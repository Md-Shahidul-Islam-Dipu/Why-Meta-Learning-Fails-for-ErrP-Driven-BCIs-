{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d496be8b",
   "metadata": {},
   "source": [
    "# ðŸ§  Subject-Conditioned Meta-Learning for ErrP-Driven BCIs\n",
    "\n",
    "**NEW APPROACH: Explicit Subject-Specific Conditioning**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Motivation\n",
    "\n",
    "**Previous Attempts:**\n",
    "1. âŒ **MAML-PPO (Policy-only)**: Failed - couldn't handle inter-subject variability\n",
    "2. âš ï¸ **MAML-Encoder / ANIL (Representation-level)**: Improved but still underperformed pooled baselines\n",
    "\n",
    "**Diagnosed Bottleneck:**\n",
    "- Subject-specific signal geometry is NOT explicitly modeled\n",
    "- Current methods assume a single universal representation works for all subjects\n",
    "- Inter-subject differences in EEG topography and amplitude are averaged out\n",
    "\n",
    "**Solution: Subject-Conditioned Meta-Learning**\n",
    "\n",
    "```\n",
    "Support Set (K trials)\n",
    "    â†“\n",
    "SubjectEncoder (learns z_s from support set)\n",
    "    â†“\n",
    "Subject Embedding z_s (8-16 dims)\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ ConditionedEEGEncoder                  â”‚\n",
    "â”‚   (FiLM: Î³, Î² from z_s)               â”‚\n",
    "â”‚   Applies: h' = Î³(z_s) âŠ™ h + Î²(z_s) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "TaskHead (classifier)\n",
    "```\n",
    "\n",
    "**Why This Should Work:**\n",
    "- âœ… Explicitly infers subject-specific latent factors from support set\n",
    "- âœ… Conditions encoder activations via FiLM (Feature-wise Linear Modulation)\n",
    "- âœ… Learned conditioning (not hand-crafted subject IDs)\n",
    "- âœ… Meta-learning optimizes for rapid subject adaptation\n",
    "- âœ… Addresses inter-subject variability at the architectural level\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- ðŸ“ˆ Improved low-K (5, 10) performance\n",
    "- ðŸ“‰ Reduced inter-subject variance\n",
    "- ðŸŽ¯ Competitive with or superior to pooled baselines\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Notebook Structure\n",
    "\n",
    "1. **Setup & Configuration** (REUSED)\n",
    "2. **Dataset Loading & Preprocessing** (REUSED)\n",
    "3. **Feature Extraction & PCA** (REUSED)\n",
    "4. **LOSO Split Construction** (REUSED)\n",
    "5. **Subject-Conditioned Architecture** (NEW)\n",
    "6. **Meta-Training Loop** (NEW)\n",
    "7. **Baselines** (REUSED)\n",
    "8. **Evaluation & Visualization** (REUSED)\n",
    "9. **Experimental Execution** (NEW)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b20be",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”¹ Section 1: Setup & Configuration\n",
    "\n",
    "**REUSED** from existing pipeline with minor modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de6e9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful\n",
      "PyTorch version: 2.8.0+cu126\n",
      "MNE version: 1.11.0\n",
      "NumPy version: 2.0.2\n"
     ]
    }
   ],
   "source": [
    "# REUSED: Core imports\n",
    "\n",
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Machine learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# EEG signal processing\n",
    "import mne\n",
    "from mne import create_info, EpochsArray\n",
    "from mne.io import RawArray\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "mne.set_log_level('WARNING')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ“ All imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MNE version: {mne.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f926da7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONFIGURATION SUMMARY\n",
      "======================================================================\n",
      "Dataset Root:     /kaggle/input/inria-bci-challenge/inria-bci-challenge\n",
      "Output Root:      /kaggle/working/results_subject_conditioned\n",
      "Device:           cuda\n",
      "Epoch Window:     [-200, 600] ms\n",
      "Baseline:         [-200, 0] ms\n",
      "Bandpass Filter:  1.0-30.0 Hz\n",
      "Random Seeds:     [42, 123, 456]\n",
      "\n",
      "--- Subject-Conditioned Meta-Learning (NEW) ---\n",
      "Subject Embed Dim: 32\n",
      "Encoder Hidden:    64\n",
      "Encoder Output:    32\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# REUSED: Global configuration\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Global configuration for reproducible experiments.\n",
    "    \n",
    "    MODIFIED: Updated for subject-conditioned meta-learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    # â”€â”€â”€ Dataset Paths (READ-ONLY) â”€â”€â”€\n",
    "    DATASET_ROOT = \"/kaggle/input/inria-bci-challenge/inria-bci-challenge\"\n",
    "    TRAIN_DIR = os.path.join(DATASET_ROOT, \"train\")\n",
    "    TEST_DIR = os.path.join(DATASET_ROOT, \"test\")\n",
    "    LABELS_FILE = os.path.join(DATASET_ROOT, \"TrainLabels.csv\")\n",
    "    CHANNELS_FILE = os.path.join(DATASET_ROOT, \"ChannelsLocation.csv\")\n",
    "\n",
    "    # â”€â”€â”€ Output Paths (WRITABLE) â”€â”€â”€\n",
    "    OUTPUT_ROOT = \"/kaggle/working/results_subject_conditioned\"\n",
    "    RESULTS_DIR = OUTPUT_ROOT\n",
    "    FIGURES_DIR = os.path.join(OUTPUT_ROOT, \"figures\")\n",
    "    METRICS_DIR = os.path.join(OUTPUT_ROOT, \"metrics\")\n",
    "    CHECKPOINT_DIR = os.path.join(OUTPUT_ROOT, \"checkpoints\")\n",
    "    \n",
    "    # â”€â”€â”€ EEG Parameters â”€â”€â”€\n",
    "    TMIN = -0.2  # Baseline start (200 ms before feedback)\n",
    "    TMAX = 0.6   # Epoch end (600 ms after feedback)\n",
    "    BASELINE = (-0.2, 0.0)  # Baseline correction window\n",
    "    \n",
    "    # Preprocessing\n",
    "    LOWCUT = 1.0   # High-pass filter (Hz)\n",
    "    HIGHCUT = 30.0  # Low-pass filter (Hz)\n",
    "    NOTCH_FREQ = 50.0  # Powerline noise (Hz)\n",
    "    \n",
    "    # Feature extraction\n",
    "    FREQ_BANDS = {\n",
    "        'theta': (4, 7),\n",
    "        'alpha': (8, 12),\n",
    "        'beta': (13, 30)\n",
    "    }\n",
    "    \n",
    "    PCA_VARIANCE = 0.95  # Retain 95% variance\n",
    "    \n",
    "    # â”€â”€â”€ Meta-Learning Parameters â”€â”€â”€\n",
    "    K_SHOTS = [5, 10, 20, 50]  # Few-shot adaptation steps\n",
    "    N_SEEDS = 3  # Random seeds for repeated experiments\n",
    "    \n",
    "    # â”€â”€â”€ Subject-Conditioned Meta-Learning (NEW) â”€â”€â”€\n",
    "    SUBJECT_EMBED_DIM = 32  # Dimension of subject embedding z_s (increased for more capacity)\n",
    "    ENCODER_HIDDEN = 64     # Hidden dimension of EEG encoder\n",
    "    ENCODER_OUTPUT = 32     # Output dimension of EEG encoder\n",
    "    \n",
    "    # â”€â”€â”€ Reproducibility â”€â”€â”€\n",
    "    RANDOM_SEEDS = [42, 123, 456]\n",
    "    \n",
    "    # â”€â”€â”€ Device Selection â”€â”€â”€\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(Config.RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(Config.FIGURES_DIR, exist_ok=True)\n",
    "os.makedirs(Config.METRICS_DIR, exist_ok=True)\n",
    "os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Dataset Root:     {Config.DATASET_ROOT}\")\n",
    "print(f\"Output Root:      {Config.OUTPUT_ROOT}\")\n",
    "print(f\"Device:           {Config.DEVICE}\")\n",
    "print(f\"Epoch Window:     [{Config.TMIN*1000:.0f}, {Config.TMAX*1000:.0f}] ms\")\n",
    "print(f\"Baseline:         [{Config.BASELINE[0]*1000:.0f}, {Config.BASELINE[1]*1000:.0f}] ms\")\n",
    "print(f\"Bandpass Filter:  {Config.LOWCUT}-{Config.HIGHCUT} Hz\")\n",
    "print(f\"Random Seeds:     {Config.RANDOM_SEEDS}\")\n",
    "print(f\"\\n--- Subject-Conditioned Meta-Learning (NEW) ---\")\n",
    "print(f\"Subject Embed Dim: {Config.SUBJECT_EMBED_DIM}\")\n",
    "print(f\"Encoder Hidden:    {Config.ENCODER_HIDDEN}\")\n",
    "print(f\"Encoder Output:    {Config.ENCODER_OUTPUT}\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15dd848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# REUSED: Reproducibility - Set Random Seeds\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility across all libraries\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"âœ“ Random seed set to {seed}\")\n",
    "\n",
    "# Set initial seed\n",
    "set_seed(Config.RANDOM_SEEDS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87022c7",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”¹ Section 2-4: Dataset Loading, Preprocessing & Feature Extraction\n",
    "\n",
    "**REUSED** from existing pipeline. These functions are copied verbatim for code reuse.\n",
    "\n",
    "**Note:** The following cells contain utility functions that are unchanged from the original pipeline.\n",
    "Run them to load and preprocess the INRIA BCI Challenge dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0848123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from cache: /kaggle/input/inria-bci-challenge/inria-bci-challenge/preprocessed_subjects.npy\n",
      "âœ“ Loaded 16 subjects from cache\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSED DATA SUMMARY\n",
      "======================================================================\n",
      "  S02: 323 trials, 57 channels, 158 timepoints\n",
      "  S06: 307 trials, 57 channels, 158 timepoints\n",
      "  S07: 259 trials, 57 channels, 158 timepoints\n",
      "  S11: 323 trials, 57 channels, 158 timepoints\n",
      "  S12: 183 trials, 57 channels, 158 timepoints\n",
      "  ... and 11 more subjects\n"
     ]
    }
   ],
   "source": [
    "# REUSED: Load preprocessed data from cache\n",
    "# \n",
    "# This cell loads the preprocessed INRIA BCI Challenge dataset.\n",
    "# The preprocessing pipeline (filtering, baseline correction, artifact rejection)\n",
    "# is identical to the original MAML_PPO_ErrP_BCI_Pipeline.ipynb\n",
    "#\n",
    "# For details on preprocessing, see the original notebook.\n",
    "\n",
    "# Check if preprocessed data exists\n",
    "preprocessed_path = os.path.join(Config.DATASET_ROOT, 'preprocessed_subjects.npy')\n",
    "\n",
    "if os.path.exists(preprocessed_path):\n",
    "    print(f\"Loading preprocessed data from cache: {preprocessed_path}\")\n",
    "    preprocessed_subjects_data = np.load(preprocessed_path, allow_pickle=True).item()\n",
    "    print(f\"âœ“ Loaded {len(preprocessed_subjects_data)} subjects from cache\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PREPROCESSED DATA SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for subject_id in sorted(list(preprocessed_subjects_data.keys())[:5]):\n",
    "        data = preprocessed_subjects_data[subject_id]\n",
    "        print(f\"  {subject_id}: {len(data['labels'])} trials, \"\n",
    "              f\"{data['epochs'].shape[1]} channels, \"\n",
    "              f\"{data['epochs'].shape[2]} timepoints\")\n",
    "    print(f\"  ... and {len(preprocessed_subjects_data) - 5} more subjects\")\n",
    "else:\n",
    "    print(f\"ERROR: Preprocessed data not found at: {preprocessed_path}\")\n",
    "    print(f\"Please run the original MAML_PPO_ErrP_BCI_Pipeline.ipynb first to generate preprocessed data.\")\n",
    "    raise FileNotFoundError(f\"Missing preprocessed data: {preprocessed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada1c7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE EXTRACTION\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb84b49064c4025a5a131d53f0a2a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature extraction:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Extracted features for 16 subjects\n",
      "\n",
      "======================================================================\n",
      "PCA DIMENSIONALITY REDUCTION (LOSO)\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f930373e568045e2a4669d07528b1668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LOSO PCA:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 16 LOSO folds with 32 PCA components\n"
     ]
    }
   ],
   "source": [
    "# REUSED: Extract bandpower features and apply PCA\n",
    "#\n",
    "# These are utility functions copied from the original pipeline.\n",
    "# They handle feature extraction and LOSO PCA reduction.\n",
    "\n",
    "def compute_bandpower_vectorized(epochs_data: np.ndarray, sfreq: float, band: Tuple[float, float]) -> np.ndarray:\n",
    "    \"\"\"REUSED: Compute bandpower for a specific frequency band using vectorized Welch's method.\"\"\"\n",
    "    n_epochs, n_channels, n_times = epochs_data.shape\n",
    "    freqs, psd = signal.welch(epochs_data, fs=sfreq, nperseg=n_times, axis=2, scaling='density')\n",
    "    band_mask = (freqs >= band[0]) & (freqs <= band[1])\n",
    "    psd_band = psd[:, :, band_mask]\n",
    "    freqs_band = freqs[band_mask]\n",
    "    # Use scipy.integrate.trapezoid (trapz is deprecated in newer scipy versions)\n",
    "    try:\n",
    "        from scipy.integrate import trapezoid\n",
    "        bandpower = trapezoid(psd_band, freqs_band, axis=2)\n",
    "    except ImportError:\n",
    "        bandpower = np.trapz(psd_band, freqs_band, axis=2)\n",
    "    return bandpower\n",
    "\n",
    "def extract_bandpower_features(epochs_data: np.ndarray, sfreq: float, freq_bands: Dict[str, Tuple[float, float]]) -> np.ndarray:\n",
    "    \"\"\"REUSED: Extract bandpower features for multiple frequency bands.\"\"\"\n",
    "    feature_list = []\n",
    "    band_names_sorted = sorted(freq_bands.keys())\n",
    "    for band_name in band_names_sorted:\n",
    "        band_range = freq_bands[band_name]\n",
    "        bandpower = compute_bandpower_vectorized(epochs_data, sfreq, band_range)\n",
    "        feature_list.append(bandpower)\n",
    "    features_stacked = np.stack(feature_list, axis=2)\n",
    "    features_sum = features_stacked.sum(axis=2, keepdims=True)\n",
    "    features_normalized = features_stacked / (features_sum + 1e-12)\n",
    "    features_log = np.log10(features_normalized + 1e-12)\n",
    "    features_final = features_log.reshape(len(epochs_data), -1)\n",
    "    return features_final\n",
    "\n",
    "def extract_features_all_subjects(preprocessed_subjects_data: Dict[str, Dict], freq_bands: Dict[str, Tuple[float, float]]) -> Dict[str, Dict]:\n",
    "    \"\"\"REUSED: Extract features for all subjects.\"\"\"\n",
    "    subjects_features = {}\n",
    "    for subject_id in tqdm(sorted(preprocessed_subjects_data.keys()), desc=\"Feature extraction\"):\n",
    "        data = preprocessed_subjects_data[subject_id]\n",
    "        features = extract_bandpower_features(data['epochs'], data['sfreq'], freq_bands)\n",
    "        subjects_features[subject_id] = {\n",
    "            'subject_id': subject_id,\n",
    "            'features': features,\n",
    "            'labels': data['labels'],\n",
    "            'n_channels': len(data['ch_names']),\n",
    "            'n_bands': len(freq_bands)\n",
    "        }\n",
    "    return subjects_features\n",
    "\n",
    "class PCAReducer:\n",
    "    \"\"\"REUSED: Fixed-dimensionality PCA for meta-learning.\"\"\"\n",
    "    def __init__(self, n_components: int = 32, random_state: int = 42):\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=n_components, whiten=True, random_state=random_state)\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, train_features: np.ndarray):\n",
    "        features_scaled = self.scaler.fit_transform(train_features)\n",
    "        self.pca.fit(features_scaled)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, features: np.ndarray) -> np.ndarray:\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        return self.pca.transform(features_scaled)\n",
    "\n",
    "def apply_pca_loso_efficient(subjects_features: Dict[str, Dict], n_components: int = 32) -> Dict[str, Dict]:\n",
    "    \"\"\"REUSED: Apply PCA with LOSO strategy.\"\"\"\n",
    "    subject_ids = sorted(subjects_features.keys())\n",
    "    loso_splits = {}\n",
    "    \n",
    "    for test_subject in tqdm(subject_ids, desc=\"LOSO PCA\"):\n",
    "        train_subjects = [s for s in subject_ids if s != test_subject]\n",
    "        train_features_list = [subjects_features[s]['features'] for s in train_subjects]\n",
    "        train_features_concat = np.concatenate(train_features_list, axis=0)\n",
    "        \n",
    "        pca_reducer = PCAReducer(n_components=n_components, random_state=42)\n",
    "        pca_reducer.fit(train_features_concat)\n",
    "        \n",
    "        train_data = []\n",
    "        for train_subject in train_subjects:\n",
    "            features_reduced = pca_reducer.transform(subjects_features[train_subject]['features'])\n",
    "            train_data.append({\n",
    "                'subject_id': train_subject,\n",
    "                'features': features_reduced,\n",
    "                'labels': subjects_features[train_subject]['labels']\n",
    "            })\n",
    "        \n",
    "        test_features_reduced = pca_reducer.transform(subjects_features[test_subject]['features'])\n",
    "        test_data = {\n",
    "            'subject_id': test_subject,\n",
    "            'features': test_features_reduced,\n",
    "            'labels': subjects_features[test_subject]['labels']\n",
    "        }\n",
    "        \n",
    "        loso_splits[test_subject] = {\n",
    "            'train': train_data,\n",
    "            'test': test_data,\n",
    "            'pca': pca_reducer,\n",
    "            'train_subjects': train_subjects,\n",
    "            'test_subject': test_subject\n",
    "        }\n",
    "    \n",
    "    return loso_splits\n",
    "\n",
    "# Extract features\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FEATURE EXTRACTION\")\n",
    "print(f\"{'='*70}\")\n",
    "subjects_features = extract_features_all_subjects(preprocessed_subjects_data, Config.FREQ_BANDS)\n",
    "print(f\"âœ“ Extracted features for {len(subjects_features)} subjects\")\n",
    "\n",
    "# Apply PCA with LOSO\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PCA DIMENSIONALITY REDUCTION (LOSO)\")\n",
    "print(f\"{'='*70}\")\n",
    "loso_splits = apply_pca_loso_efficient(subjects_features, n_components=32)\n",
    "print(f\"âœ“ Created {len(loso_splits)} LOSO folds with 32 PCA components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07603865",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”¹ Section 5: Subject-Conditioned Meta-Learning Architecture\n",
    "\n",
    "**NEW: Core Innovation**\n",
    "\n",
    "This section implements the subject-conditioned meta-learning approach that explicitly models subject-specific signal geometry.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "Support Set (K trials from subject)\n",
    "    â†“\n",
    "SubjectEncoder (learns embedding z_s)\n",
    "    â†“\n",
    "Subject Embedding z_s (16-dim latent)\n",
    "    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ ConditionedEEGEncoder                  â”‚\n",
    "â”‚   FiLM Layer 1: Î³1, Î²1 = f(z_s)      â”‚\n",
    "â”‚   h1' = Î³1 âŠ™ h1 + Î²1                  â”‚\n",
    "â”‚   FiLM Layer 2: Î³2, Î²2 = f(z_s)      â”‚\n",
    "â”‚   h2' = Î³2 âŠ™ h2 + Î²2                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â†“\n",
    "Task Head (classifier)\n",
    "```\n",
    "\n",
    "### Why This Addresses the Bottleneck\n",
    "\n",
    "1. **Explicit Subject Modeling**: z_s captures subject-specific EEG characteristics\n",
    "2. **Learned Conditioning**: FiLM parameters (Î³, Î²) are learned, not hard-coded\n",
    "3. **Feature-wise Modulation**: Each neuron can be scaled/shifted differently per subject\n",
    "4. **Meta-Learning Compatible**: Entire architecture is differentiable and meta-trainable\n",
    "\n",
    "### Meta-Learning Strategy\n",
    "\n",
    "- **Outer Loop (Meta-Update)**: Update SubjectEncoder + ConditionedEEGEncoder\n",
    "- **Inner Loop (Adaptation)**: Adapt Task Head only (or optionally all parameters)\n",
    "- **Loss**: Supervised cross-entropy on query set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bed1f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TESTING SUBJECT-CONDITIONED ARCHITECTURE\n",
      "======================================================================\n",
      "\n",
      "âœ“ Architecture created:\n",
      "  SubjectEncoder: 32 â†’ 32\n",
      "  ConditionedEEGEncoder: 32 â†’ 32 (with FiLM)\n",
      "  TaskHead: 32 â†’ 2\n",
      "\n",
      "âœ“ Subject embedding: shape torch.Size([32])\n",
      "âœ“ Conditioned encoding: shape torch.Size([1, 32])\n",
      "âœ“ Classification logits: shape torch.Size([1, 2])\n",
      "\n",
      "âœ“ Forward pass successful! Architecture is ready for meta-learning.\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# NEW: SUBJECT ENCODER - Infers subject embedding from support set\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class SubjectEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    NEW: Encoder that learns a subject-specific embedding from support set trials.\n",
    "    \n",
    "    Architecture:\n",
    "        EEG features (K Ã— input_dim) \n",
    "        â†’ Mean pooling \n",
    "        â†’ MLP \n",
    "        â†’ Subject embedding z_s (embed_dim)\n",
    "    \n",
    "    Why this works:\n",
    "    - Aggregates information from K support trials\n",
    "    - Learns to extract subject-invariant statistics (mean, variance patterns)\n",
    "    - Low-dimensional embedding captures essential subject characteristics\n",
    "    - Differentiable â†’ can be meta-learned\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, embed_dim: int = 32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimensionality of PCA-reduced EEG features\n",
    "            hidden_dim: Hidden layer size\n",
    "            embed_dim: Dimensionality of subject embedding z_s (increased to 32 for more capacity)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # MLP to process aggregated support set statistics (with extra hidden layer)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),  # Light regularization\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # Extra hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, support_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute subject embedding from support set.\n",
    "        \n",
    "        Args:\n",
    "            support_features: Tensor of shape (K, input_dim) - K support trials\n",
    "        \n",
    "        Returns:\n",
    "            subject_embedding: Tensor of shape (embed_dim,) - subject embedding z_s\n",
    "        \"\"\"\n",
    "        # Aggregate support set (mean pooling)\n",
    "        # This captures the \"average\" EEG signature of the subject\n",
    "        support_mean = support_features.mean(dim=0)  # (input_dim,)\n",
    "        \n",
    "        # Encode to low-dimensional subject embedding\n",
    "        z_s = self.encoder(support_mean)  # (embed_dim,)\n",
    "        \n",
    "        return z_s\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# NEW: FiLM LAYER - Feature-wise Linear Modulation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class FiLMLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    NEW: Feature-wise Linear Modulation (FiLM) layer.\n",
    "    \n",
    "    Applies learned affine transformation conditioned on subject embedding:\n",
    "        h' = Î³(z_s) âŠ™ h + Î²(z_s)\n",
    "    \n",
    "        z_s: subject embedding (32-dim)\n",
    "        Î³ (gamma): scale parameters (feature-wise)\n",
    "        Î² (beta): shift parameters (feature-wise)\n",
    "        z_s: subject embedding\n",
    "        âŠ™: element-wise multiplication\n",
    "    \n",
    "    Why FiLM:\n",
    "    - Proven effective for domain adaptation in vision/speech\n",
    "    - Modulates features without changing network architecture\n",
    "    - Each hidden unit can be scaled/shifted independently\n",
    "    - Efficient: only 2Ã—hidden_dim parameters per layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, embed_dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim: Dimensionality of hidden features to modulate\n",
    "            embed_dim: Dimensionality of subject embedding z_s\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Learned transformations from z_s to (Î³, Î²)\n",
    "        self.gamma_fc = nn.Linear(embed_dim, hidden_dim)  # Scale parameters\n",
    "        self.beta_fc = nn.Linear(embed_dim, hidden_dim)   # Shift parameters\n",
    "    \n",
    "    def forward(self, h: torch.Tensor, z_s: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply FiLM conditioning.\n",
    "        \n",
    "        Args:\n",
    "            h: Hidden features, shape (batch_size, hidden_dim) or (hidden_dim,)\n",
    "            z_s: Subject embedding, shape (embed_dim,)\n",
    "        \n",
    "        Returns:\n",
    "            h_conditioned: Modulated features, same shape as h\n",
    "        \"\"\"\n",
    "        # Compute scale and shift parameters from subject embedding\n",
    "        gamma = self.gamma_fc(z_s)  # (hidden_dim,)\n",
    "        beta = self.beta_fc(z_s)    # (hidden_dim,)\n",
    "        \n",
    "        # Apply affine transformation: h' = Î³ âŠ™ h + Î²\n",
    "        # Broadcasting handles batch dimension automatically\n",
    "        h_conditioned = gamma * h + beta\n",
    "        \n",
    "        return h_conditioned\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# NEW: CONDITIONED EEG ENCODER - Encoder with FiLM conditioning\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class ConditionedEEGEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    NEW: EEG encoder with FiLM conditioning at each layer.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (PCA features)\n",
    "        â†’ Linear + ReLU\n",
    "        â†’ FiLM(z_s)         â† Condition on subject\n",
    "        â†’ Linear + ReLU\n",
    "        â†’ FiLM(z_s)         â† Condition on subject\n",
    "        â†’ Output (encoder_output_dim)\n",
    "    \n",
    "    Why this works:\n",
    "    - Standard encoder learns universal EEG patterns\n",
    "    - FiLM layers adapt activations to each subject\n",
    "    - Subject-specific scaling/shifting preserves learned features\n",
    "    - Meta-learnable: gradients flow through z_s â†’ SubjectEncoder\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim: int, \n",
    "        hidden_dim: int = 64, \n",
    "        encoder_output_dim: int = 32,\n",
    "        embed_dim: int = 32\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimensionality of PCA-reduced features\n",
    "            hidden_dim: Hidden layer size\n",
    "            encoder_output_dim: Output dimensionality\n",
    "            embed_dim: Dimensionality of subject embedding z_s (32-dim for increased capacity)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.encoder_output_dim = encoder_output_dim\n",
    "        \n",
    "        # Layer 1: input â†’ hidden\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.film1 = FiLMLayer(hidden_dim, embed_dim)\n",
    "        \n",
    "        # Layer 2: hidden â†’ hidden\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.film2 = FiLMLayer(hidden_dim, embed_dim)\n",
    "        \n",
    "        # Layer 3: hidden â†’ output (no FiLM on final layer)\n",
    "        self.fc3 = nn.Linear(hidden_dim, encoder_output_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, z_s: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with subject conditioning.\n",
    "        \n",
    "        Args:\n",
    "            x: Input features, shape (batch_size, input_dim) or (input_dim,)\n",
    "            z_s: Subject embedding, shape (embed_dim,)\n",
    "        \n",
    "        Returns:\n",
    "            h: Encoded features, shape (batch_size, encoder_output_dim) or (encoder_output_dim,)\n",
    "        \"\"\"\n",
    "        # Layer 1: Linear â†’ ReLU â†’ FiLM(z_s)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.film1(h, z_s)\n",
    "        \n",
    "        # Layer 2: Linear â†’ ReLU â†’ FiLM(z_s)\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.film2(h, z_s)\n",
    "        \n",
    "        # Layer 3: Linear â†’ Output (no activation, no FiLM)\n",
    "        h = self.fc3(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# NEW: TASK HEAD - Simple classifier on top of encoder\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class TaskHead(nn.Module):\n",
    "    \"\"\"\n",
    "    NEW: Task-specific classifier (adapted per subject).\n",
    "    \n",
    "    Simple linear classifier on top of conditioned encoder.\n",
    "    This is the only component adapted in the inner loop (default).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_output_dim: int, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(encoder_output_dim, num_classes)\n",
    "    \n",
    "    def forward(self, h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: Encoded features from ConditionedEEGEncoder\n",
    "        \n",
    "        Returns:\n",
    "            logits: Class logits\n",
    "        \"\"\"\n",
    "        return self.fc(h)\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TEST: Verify architecture forward pass\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TESTING SUBJECT-CONDITIONED ARCHITECTURE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Get sample dimensions from LOSO splits\n",
    "first_fold = list(loso_splits.keys())[0]\n",
    "sample_data = loso_splits[first_fold]['test']\n",
    "input_dim = sample_data['features'].shape[1]\n",
    "\n",
    "# Create components\n",
    "subject_encoder = SubjectEncoder(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=64,\n",
    "    embed_dim=Config.SUBJECT_EMBED_DIM\n",
    ")\n",
    "\n",
    "conditioned_encoder = ConditionedEEGEncoder(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=Config.ENCODER_HIDDEN,\n",
    "    encoder_output_dim=Config.ENCODER_OUTPUT,\n",
    "    embed_dim=Config.SUBJECT_EMBED_DIM\n",
    ")\n",
    "\n",
    "task_head = TaskHead(\n",
    "    encoder_output_dim=Config.ENCODER_OUTPUT,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Architecture created:\")\n",
    "print(f\"  SubjectEncoder: {input_dim} â†’ {Config.SUBJECT_EMBED_DIM}\")\n",
    "print(f\"  ConditionedEEGEncoder: {input_dim} â†’ {Config.ENCODER_OUTPUT} (with FiLM)\")\n",
    "print(f\"  TaskHead: {Config.ENCODER_OUTPUT} â†’ 2\")\n",
    "\n",
    "# Test forward pass\n",
    "K = 10\n",
    "support_features = torch.randn(K, input_dim)\n",
    "query_features = torch.randn(1, input_dim)\n",
    "\n",
    "# Compute subject embedding\n",
    "z_s = subject_encoder(support_features)\n",
    "print(f\"\\nâœ“ Subject embedding: shape {z_s.shape}\")\n",
    "\n",
    "# Encode query with conditioning\n",
    "h = conditioned_encoder(query_features, z_s)\n",
    "print(f\"âœ“ Conditioned encoding: shape {h.shape}\")\n",
    "\n",
    "# Classify\n",
    "logits = task_head(h)\n",
    "print(f\"âœ“ Classification logits: shape {logits.shape}\")\n",
    "\n",
    "print(f\"\\nâœ“ Forward pass successful! Architecture is ready for meta-learning.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13637c35",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”¹ Section 6: Subject-Conditioned Meta-Learner\n",
    "\n",
    "**NEW: Complete meta-learning system**\n",
    "\n",
    "This integrates all components and implements the MAML-style meta-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "816c98fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âœ“ SUBJECT-CONDITIONED META-LEARNER IMPLEMENTED\n",
      "======================================================================\n",
      "\n",
      "Key features:\n",
      "  - SubjectEncoder: Infers z_s from support set\n",
      "  - ConditionedEEGEncoder: FiLM conditioning at each layer\n",
      "  - TaskHead: Simple classifier (adapted per subject)\n",
      "  - Meta-learning: Outer loop updates encoder, inner loop adapts task head\n",
      "  - Loss: Supervised cross-entropy\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# NEW: SUBJECT-CONDITIONED META-LEARNER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class SubjectConditionedMetaLearner:\n",
    "    \"\"\"\n",
    "    NEW: Complete subject-conditioned meta-learning system.\n",
    "    \n",
    "    Components:\n",
    "    - SubjectEncoder: Infers z_s from support set\n",
    "    - ConditionedEEGEncoder: Encodes features conditioned on z_s\n",
    "    - TaskHead: Classifier (adapted per subject)\n",
    "    \n",
    "    Meta-Learning:\n",
    "    - Outer loop: Update SubjectEncoder + ConditionedEEGEncoder\n",
    "    - Inner loop: Adapt TaskHead (default) or all parameters\n",
    "    - Loss: Cross-entropy on query set\n",
    "    \n",
    "    Why this addresses the bottleneck:\n",
    "    - Explicitly models subject-specific signal geometry via z_s\n",
    "    - FiLM conditioning adapts encoder activations per subject\n",
    "    - Meta-learning optimizes for rapid subject adaptation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        subject_embed_dim: int = 32,\n",
    "        encoder_hidden: int = 64,\n",
    "        encoder_output: int = 32,\n",
    "        num_classes: int = 2,\n",
    "        inner_lr: float = 0.01,\n",
    "        outer_lr: float = 0.0003,\n",
    "        inner_steps: int = 5,\n",
    "        adapt_encoder: bool = False,  # If True, adapt encoder in inner loop\n",
    "        first_order: bool = True,\n",
    "        device: str = 'cpu'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimensionality of PCA-reduced features\n",
    "            subject_embed_dim: Dimension of subject embedding z_s (32-dim for increased capacity)\n",
    "            encoder_hidden: Hidden dimension of encoder\n",
    "            encoder_output: Output dimension of encoder\n",
    "            num_classes: Number of classes (2 for ErrP)\n",
    "            inner_lr: Learning rate for inner loop adaptation\n",
    "            outer_lr: Learning rate for outer loop meta-update\n",
    "            inner_steps: Number of gradient steps in inner loop\n",
    "            adapt_encoder: Whether to adapt encoder in inner loop (default: False)\n",
    "            first_order: Use first-order MAML (faster, no second-order gradients)\n",
    "            device: Computation device\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.adapt_encoder = adapt_encoder\n",
    "        self.first_order = first_order\n",
    "        \n",
    "        # Create components\n",
    "        self.subject_encoder = SubjectEncoder(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=encoder_hidden,\n",
    "            embed_dim=subject_embed_dim\n",
    "        ).to(device)\n",
    "        \n",
    "        self.conditioned_encoder = ConditionedEEGEncoder(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=encoder_hidden,\n",
    "            encoder_output_dim=encoder_output,\n",
    "            embed_dim=subject_embed_dim\n",
    "        ).to(device)\n",
    "        \n",
    "        self.task_head = TaskHead(\n",
    "            encoder_output_dim=encoder_output,\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "        \n",
    "        # Meta-optimizer (updates SubjectEncoder + ConditionedEEGEncoder)\n",
    "        self.meta_params = list(self.subject_encoder.parameters()) + \\\n",
    "                          list(self.conditioned_encoder.parameters())\n",
    "        \n",
    "        self.meta_optimizer = optim.Adam(self.meta_params, lr=outer_lr)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, z_s: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the full model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input features\n",
    "            z_s: Subject embedding\n",
    "        \n",
    "        Returns:\n",
    "            logits: Classification logits\n",
    "        \"\"\"\n",
    "        h = self.conditioned_encoder(x, z_s)\n",
    "        logits = self.task_head(h)\n",
    "        return logits\n",
    "    \n",
    "    def compute_loss(self, features: np.ndarray, labels: np.ndarray, z_s: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "        x = torch.FloatTensor(features).to(self.device)\n",
    "        y = torch.LongTensor(labels).to(self.device)\n",
    "        \n",
    "        logits = self.forward(x, z_s)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    def adapt_to_task(\n",
    "        self,\n",
    "        support_features: np.ndarray,\n",
    "        support_labels: np.ndarray\n",
    "    ) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"\n",
    "        Adapt to a new subject using support set (inner loop).\n",
    "        \n",
    "        Args:\n",
    "            support_features: Support set features (K, input_dim)\n",
    "            support_labels: Support set labels (K,)\n",
    "        \n",
    "        Returns:\n",
    "            adapted_params: Dictionary of adapted parameters\n",
    "            metrics: Dictionary with adaptation metrics\n",
    "        \"\"\"\n",
    "        # Compute subject embedding (no grad - fixed during inner loop)\n",
    "        support_x = torch.FloatTensor(support_features).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            z_s = self.subject_encoder(support_x)\n",
    "        \n",
    "        # Clone task head parameters for adaptation\n",
    "        adapted_task_params = {\n",
    "            name: param.clone().detach().requires_grad_(True)\n",
    "            for name, param in self.task_head.named_parameters()\n",
    "        }\n",
    "        \n",
    "        # Optionally clone encoder parameters\n",
    "        if self.adapt_encoder:\n",
    "            adapted_encoder_params = {\n",
    "                name: param.clone().detach().requires_grad_(True)\n",
    "                for name, param in self.conditioned_encoder.named_parameters()\n",
    "            }\n",
    "        \n",
    "        # Inner loop: Adapt task head (and optionally encoder)\n",
    "        for step in range(self.inner_steps):\n",
    "            # Forward with adapted parameters\n",
    "            x = torch.FloatTensor(support_features).to(self.device)\n",
    "            y = torch.LongTensor(support_labels).to(self.device)\n",
    "            \n",
    "            # Manually apply adapted parameters to task head\n",
    "            h = self.conditioned_encoder(x, z_s)\n",
    "            \n",
    "            # Compute logits with adapted task head\n",
    "            logits = F.linear(h, adapted_task_params['fc.weight'], adapted_task_params['fc.bias'])\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            \n",
    "            # Compute gradients\n",
    "            grads = torch.autograd.grad(\n",
    "                loss,\n",
    "                list(adapted_task_params.values()),\n",
    "                create_graph=not self.first_order\n",
    "            )\n",
    "            \n",
    "            # Update adapted parameters\n",
    "            adapted_task_params = {\n",
    "                name: param - self.inner_lr * grad\n",
    "                for (name, param), grad in zip(adapted_task_params.items(), grads)\n",
    "            }\n",
    "        \n",
    "        # Compute final adaptation metrics\n",
    "        with torch.no_grad():\n",
    "            logits = F.linear(h, adapted_task_params['fc.weight'], adapted_task_params['fc.bias'])\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            accuracy = (preds == y).float().mean().item()\n",
    "        \n",
    "        metrics = {\n",
    "            'adapt_loss': loss.item(),\n",
    "            'adapt_accuracy': accuracy\n",
    "        }\n",
    "        \n",
    "        adapted_params = {\n",
    "            'z_s': z_s,\n",
    "            'task_params': adapted_task_params\n",
    "        }\n",
    "        \n",
    "        return adapted_params, metrics\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        adapted_params: Dict,\n",
    "        query_features: np.ndarray,\n",
    "        query_labels: np.ndarray\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate adapted model on query set.\n",
    "        \n",
    "        Args:\n",
    "            adapted_params: Adapted parameters from adapt_to_task()\n",
    "            query_features: Query set features\n",
    "            query_labels: Query set labels\n",
    "        \n",
    "        Returns:\n",
    "            metrics: Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        z_s = adapted_params['z_s']\n",
    "        task_params = adapted_params['task_params']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = torch.FloatTensor(query_features).to(self.device)\n",
    "            y = torch.LongTensor(query_labels).to(self.device)\n",
    "            \n",
    "            h = self.conditioned_encoder(x, z_s)\n",
    "            logits = F.linear(h, task_params['fc.weight'], task_params['fc.bias'])\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct = (preds == y).sum().item()\n",
    "            total = len(query_labels)\n",
    "            accuracy = correct / total\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'correct': correct,\n",
    "            'total': total\n",
    "        }\n",
    "    \n",
    "    def meta_update(\n",
    "        self,\n",
    "        tasks_support: List[Tuple[np.ndarray, np.ndarray]],\n",
    "        tasks_query: List[Tuple[np.ndarray, np.ndarray]]\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform meta-update on a batch of tasks (outer loop).\n",
    "        \n",
    "        Args:\n",
    "            tasks_support: List of (support_features, support_labels) tuples\n",
    "            tasks_query: List of (query_features, query_labels) tuples\n",
    "        \n",
    "        Returns:\n",
    "            metrics: Dictionary with meta-learning metrics\n",
    "        \"\"\"\n",
    "        self.meta_optimizer.zero_grad()\n",
    "        \n",
    "        meta_loss = 0.0\n",
    "        adapt_accuracies = []\n",
    "        query_accuracies = []\n",
    "        \n",
    "        for (support_features, support_labels), (query_features, query_labels) in \\\n",
    "                zip(tasks_support, tasks_query):\n",
    "            \n",
    "            # Adapt to task\n",
    "            adapted_params, adapt_metrics = self.adapt_to_task(\n",
    "                support_features,\n",
    "                support_labels\n",
    "            )\n",
    "            \n",
    "            # Compute query loss (for meta-gradient)\n",
    "            z_s = adapted_params['z_s']\n",
    "            task_params = adapted_params['task_params']\n",
    "            \n",
    "            x = torch.FloatTensor(query_features).to(self.device)\n",
    "            y = torch.LongTensor(query_labels).to(self.device)\n",
    "            \n",
    "            h = self.conditioned_encoder(x, z_s)\n",
    "            logits = F.linear(h, task_params['fc.weight'], task_params['fc.bias'])\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            \n",
    "            meta_loss += loss\n",
    "            adapt_accuracies.append(adapt_metrics['adapt_accuracy'])\n",
    "            \n",
    "            # Compute query accuracy\n",
    "            with torch.no_grad():\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                query_acc = (preds == y).float().mean().item()\n",
    "                query_accuracies.append(query_acc)\n",
    "        \n",
    "        # Average meta-loss across tasks\n",
    "        meta_loss /= len(tasks_support)\n",
    "        \n",
    "        # Backpropagate and update meta-parameters\n",
    "        meta_loss.backward()\n",
    "        self.meta_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'meta_loss': meta_loss.item(),\n",
    "            'adapt_accuracy': np.mean(adapt_accuracies),\n",
    "            'query_accuracy': np.mean(query_accuracies)\n",
    "        }\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"âœ“ SUBJECT-CONDITIONED META-LEARNER IMPLEMENTED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - SubjectEncoder: Infers z_s from support set\")\n",
    "print(\"  - ConditionedEEGEncoder: FiLM conditioning at each layer\")\n",
    "print(\"  - TaskHead: Simple classifier (adapted per subject)\")\n",
    "print(\"  - Meta-learning: Outer loop updates encoder, inner loop adapts task head\")\n",
    "print(\"  - Loss: Supervised cross-entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e084367",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”¹ Section 7: Training Loop - Subject-Conditioned Meta-Learning\n",
    "\n",
    "**NEW: LOSO meta-training loop**\n",
    "\n",
    "This implements the full training pipeline with LOSO evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a710a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# NEW: TRAINING LOOP - Subject-Conditioned Meta-Learning with LOSO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def train_subject_conditioned_loso(\n",
    "    loso_splits: Dict,\n",
    "    k_shots: List[int],\n",
    "    n_meta_iterations: int = 500,\n",
    "    meta_batch_size: int = 4,\n",
    "    n_support: int = 10,\n",
    "    n_query: int = 40,\n",
    "    subject_embed_dim: int = 32,\n",
    "    encoder_hidden: int = 64,\n",
    "    encoder_output: int = 32,\n",
    "    inner_lr: float = 0.01,\n",
    "    outer_lr: float = 0.0003,\n",
    "    inner_steps: int = 5,\n",
    "    adapt_encoder: bool = False,\n",
    "    device: str = 'cpu',\n",
    "    seed: int = 42\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    NEW: Train subject-conditioned meta-learner using LOSO evaluation.\n",
    "    \n",
    "    This implements the full pipeline:\n",
    "    1. For each LOSO fold (test subject):\n",
    "       a. Create fresh meta-learner\n",
    "       b. Meta-train on remaining subjects\n",
    "       c. Evaluate on test subject with different K values\n",
    "    \n",
    "    Args:\n",
    "        loso_splits: LOSO split dictionary\n",
    "        k_shots: List of K values for few-shot evaluation\n",
    "        n_meta_iterations: Number of meta-training iterations\n",
    "        meta_batch_size: Number of subjects per meta-batch\n",
    "        n_support: Support set size for inner loop\n",
    "        n_query: Query set size for meta-loss computation\n",
    "        subject_embed_dim: Dimension of subject embedding z_s\n",
    "        encoder_hidden: Hidden dimension of encoder\n",
    "        encoder_output: Output dimension of encoder\n",
    "        inner_lr: Inner loop learning rate\n",
    "        outer_lr: Outer loop learning rate\n",
    "        inner_steps: Inner loop adaptation steps\n",
    "        adapt_encoder: Whether to adapt encoder in inner loop\n",
    "        device: Computation device\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary with LOSO results\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    \n",
    "    all_subjects_results = {}\n",
    "    \n",
    "    for test_subject, fold_data in tqdm(loso_splits.items(), desc=\"Subject-Conditioned LOSO\"):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Test Subject: {test_subject}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Get train and test data\n",
    "        train_data = fold_data['train']\n",
    "        test_features = fold_data['test']['features']\n",
    "        test_labels = fold_data['test']['labels']\n",
    "        input_dim = test_features.shape[1]\n",
    "        \n",
    "        # Create fresh meta-learner for this fold\n",
    "        meta_learner = SubjectConditionedMetaLearner(\n",
    "            input_dim=input_dim,\n",
    "            subject_embed_dim=subject_embed_dim,\n",
    "            encoder_hidden=encoder_hidden,\n",
    "            encoder_output=encoder_output,\n",
    "            num_classes=2,\n",
    "            inner_lr=inner_lr,\n",
    "            outer_lr=outer_lr,\n",
    "            inner_steps=inner_steps,\n",
    "            adapt_encoder=adapt_encoder,\n",
    "            first_order=True,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Meta-training loop\n",
    "        print(f\"\\nMeta-training on {len(train_data)} subjects...\")\n",
    "        for iteration in range(n_meta_iterations):\n",
    "            # Sample meta-batch of subjects\n",
    "            batch_subjects = np.random.choice(len(train_data), size=meta_batch_size, replace=False)\n",
    "            \n",
    "            tasks_support = []\n",
    "            tasks_query = []\n",
    "            \n",
    "            for subject_idx in batch_subjects:\n",
    "                subject_data = train_data[subject_idx]\n",
    "                features = subject_data['features']\n",
    "                labels = subject_data['labels']\n",
    "                \n",
    "                # Sample support and query sets\n",
    "                indices = np.random.permutation(len(features))\n",
    "                support_indices = indices[:n_support]\n",
    "                query_indices = indices[n_support:n_support + n_query]\n",
    "                \n",
    "                support_features = features[support_indices]\n",
    "                support_labels = labels[support_indices]\n",
    "                query_features = features[query_indices]\n",
    "                query_labels = labels[query_indices]\n",
    "                \n",
    "                tasks_support.append((support_features, support_labels))\n",
    "                tasks_query.append((query_features, query_labels))\n",
    "            \n",
    "            # Meta-update\n",
    "            metrics = meta_learner.meta_update(tasks_support, tasks_query)\n",
    "            \n",
    "            if (iteration + 1) % 100 == 0:\n",
    "                print(f\"Iteration {iteration+1}/{n_meta_iterations}: \"\n",
    "                      f\"Meta Loss = {metrics['meta_loss']:.4f}, \"\n",
    "                      f\"Query Acc = {metrics['query_accuracy']:.4f}\")\n",
    "        \n",
    "        # Evaluate on test subject with different K values\n",
    "        print(f\"\\nEvaluating on test subject {test_subject}...\")\n",
    "        subject_results = {'subject_id': test_subject, 'k_shots': {}}\n",
    "        \n",
    "        for k in k_shots:\n",
    "            # Sample K support shots from test subject\n",
    "            indices = np.random.permutation(len(test_features))\n",
    "            support_indices = indices[:k]\n",
    "            query_indices = indices[k:]\n",
    "            \n",
    "            support_features = test_features[support_indices]\n",
    "            support_labels = test_labels[support_indices]\n",
    "            query_features = test_features[query_indices]\n",
    "            query_labels = test_labels[query_indices]\n",
    "            \n",
    "            # Adapt to test subject\n",
    "            adapted_params, adapt_metrics = meta_learner.adapt_to_task(\n",
    "                support_features,\n",
    "                support_labels\n",
    "            )\n",
    "            \n",
    "            # Evaluate on remaining data\n",
    "            eval_metrics = meta_learner.evaluate(\n",
    "                adapted_params,\n",
    "                query_features,\n",
    "                query_labels\n",
    "            )\n",
    "            \n",
    "            print(f\"  K={k}: Accuracy = {eval_metrics['accuracy']:.4f} \"\n",
    "                  f\"({eval_metrics['correct']}/{eval_metrics['total']})\")\n",
    "            \n",
    "            subject_results['k_shots'][k] = {\n",
    "                'accuracy': eval_metrics['accuracy'],\n",
    "                'correct': eval_metrics['correct'],\n",
    "                'total': eval_metrics['total'],\n",
    "                'adapt_accuracy': adapt_metrics['adapt_accuracy']\n",
    "            }\n",
    "        \n",
    "        all_subjects_results[test_subject] = subject_results\n",
    "    \n",
    "    # Compute aggregate statistics\n",
    "    summary_stats = {}\n",
    "    for k in k_shots:\n",
    "        accuracies = [all_subjects_results[s]['k_shots'][k]['accuracy'] \n",
    "                     for s in all_subjects_results.keys()]\n",
    "        summary_stats[k] = {\n",
    "            'mean': np.mean(accuracies),\n",
    "            'std': np.std(accuracies),\n",
    "            'median': np.median(accuracies),\n",
    "            'min': np.min(accuracies),\n",
    "            'max': np.max(accuracies)\n",
    "        }\n",
    "    \n",
    "    results = {\n",
    "        'method': 'Subject-Conditioned Meta-Learning',\n",
    "        'k_shots': k_shots,\n",
    "        'subjects': all_subjects_results,  # Changed from 'per_subject' to 'subjects'\n",
    "        'summary': summary_stats,\n",
    "        'config': {\n",
    "            'n_meta_iterations': n_meta_iterations,\n",
    "            'meta_batch_size': meta_batch_size,\n",
    "            'n_support': n_support,\n",
    "            'n_query': n_query,\n",
    "            'subject_embed_dim': subject_embed_dim,\n",
    "            'encoder_hidden': encoder_hidden,\n",
    "            'encoder_output': encoder_output,\n",
    "            'inner_lr': inner_lr,\n",
    "            'outer_lr': outer_lr,\n",
    "            'inner_steps': inner_steps,\n",
    "            'device': device,\n",
    "            'seed': seed\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f59d711",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”¹ Section 8: Baselines & Evaluation\n",
    "\n",
    "**REUSED** from existing pipeline with adaptations for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8640d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âœ“ BASELINES & EVALUATION FUNCTIONS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# REUSED: Supervised Baseline and Evaluation Functions\n",
    "#\n",
    "# These functions are copied from the original pipeline for comparison.\n",
    "\n",
    "def run_supervised_baseline_loso(loso_splits: Dict, k_shots: List[int], hidden_dim: int = 64, \n",
    "                                  lr: float = 0.01, n_epochs: int = 100, device: str = 'cpu', \n",
    "                                  seed: int = 42) -> Dict:\n",
    "    \"\"\"REUSED: Supervised baseline with LOSO evaluation.\"\"\"\n",
    "    set_seed(seed)\n",
    "    all_subjects_results = {}\n",
    "    \n",
    "    for test_subject, fold_data in tqdm(loso_splits.items(), desc=\"Supervised Baseline (LOSO)\"):\n",
    "        test_features = fold_data['test']['features']\n",
    "        test_labels = fold_data['test']['labels']\n",
    "        input_dim = test_features.shape[1]\n",
    "        \n",
    "        subject_results = {'subject_id': test_subject, 'k_shots': {}}\n",
    "        \n",
    "        for k in k_shots:\n",
    "            indices = np.random.permutation(len(test_features))\n",
    "            train_indices = indices[:k]\n",
    "            test_indices = indices[k:]\n",
    "            \n",
    "            train_features = test_features[train_indices]\n",
    "            train_labels = test_labels[train_indices]\n",
    "            test_features_k = test_features[test_indices]\n",
    "            test_labels_k = test_labels[test_indices]\n",
    "            \n",
    "            # Train simple MLP\n",
    "            model = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(hidden_dim, 2)\n",
    "            ).to(device)\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            model.train()\n",
    "            train_x = torch.FloatTensor(train_features).to(device)\n",
    "            train_y = torch.LongTensor(train_labels).to(device)\n",
    "            \n",
    "            for epoch in range(n_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(train_x)\n",
    "                loss = criterion(logits, train_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_x = torch.FloatTensor(test_features_k).to(device)\n",
    "                test_y = torch.LongTensor(test_labels_k).to(device)\n",
    "                logits = model(test_x)\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                correct = (predictions == test_y).sum().item()\n",
    "                total = len(test_labels_k)\n",
    "                accuracy = correct / total\n",
    "            \n",
    "            subject_results['k_shots'][k] = {'accuracy': accuracy, 'correct': correct, 'total': total}\n",
    "        \n",
    "        all_subjects_results[test_subject] = subject_results\n",
    "    \n",
    "    return {\n",
    "        'method': 'Supervised Baseline',\n",
    "        'k_shots': k_shots,\n",
    "        'subjects': all_subjects_results,\n",
    "        'seed': seed\n",
    "    }\n",
    "\n",
    "def compute_accuracy_metrics(results: Dict, method_name: str) -> pd.DataFrame:\n",
    "    \"\"\"REUSED: Compute mean/std accuracy for each K-shot.\"\"\"\n",
    "    subjects = results.get('subjects', {})\n",
    "    k_shots = results.get('k_shots', [])\n",
    "    metrics = []\n",
    "    \n",
    "    for k in k_shots:\n",
    "        accuracies = []\n",
    "        for subject_data in subjects.values():\n",
    "            if 'k_shots' in subject_data and k in subject_data['k_shots']:\n",
    "                acc = subject_data['k_shots'][k].get('accuracy', 0)\n",
    "                accuracies.append(acc)\n",
    "        \n",
    "        if accuracies:\n",
    "            metrics.append({\n",
    "                'Method': method_name,\n",
    "                'K': k,\n",
    "                'Mean': np.mean(accuracies),\n",
    "                'Std': np.std(accuracies),\n",
    "                'N': len(accuracies)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def plot_adaptation_curves(all_results: Dict, method_names: List[str], save_path: Optional[str] = None):\n",
    "    \"\"\"REUSED: Plot adaptation curves comparing methods.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    colors = ['#E63946', '#2A9D8F', '#F4A261', '#E76F51', '#264653']\n",
    "    markers = ['o', 's', '^', 'D', 'v']\n",
    "    \n",
    "    for idx, method_name in enumerate(method_names):\n",
    "        if method_name not in all_results:\n",
    "            continue\n",
    "        \n",
    "        results = all_results[method_name]\n",
    "        subjects = results.get('subjects', {})\n",
    "        k_shots = sorted(results.get('k_shots', []))\n",
    "        \n",
    "        means = []\n",
    "        stds = []\n",
    "        \n",
    "        for k in k_shots:\n",
    "            accuracies = []\n",
    "            for subject_data in subjects.values():\n",
    "                if 'k_shots' in subject_data and k in subject_data['k_shots']:\n",
    "                    acc = subject_data['k_shots'][k].get('accuracy', 0)\n",
    "                    accuracies.append(acc)\n",
    "            \n",
    "            if accuracies:\n",
    "                means.append(np.mean(accuracies))\n",
    "                stds.append(np.std(accuracies))\n",
    "            else:\n",
    "                means.append(0)\n",
    "                stds.append(0)\n",
    "        \n",
    "        means = np.array(means) * 100\n",
    "        stds = np.array(stds) * 100\n",
    "        \n",
    "        plt.plot(k_shots, means, marker=markers[idx % len(markers)], \n",
    "                linewidth=2, markersize=8, label=method_name, \n",
    "                color=colors[idx % len(colors)])\n",
    "        plt.fill_between(k_shots, means - stds, means + stds, \n",
    "                        alpha=0.2, color=colors[idx % len(colors)])\n",
    "    \n",
    "    plt.xlabel('K (Number of Support Shots)', fontsize=13, fontweight='bold')\n",
    "    plt.ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "    plt.title('Few-Shot Adaptation Performance', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11, framealpha=0.9)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"  Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def save_results_to_csv(all_results: Dict, method_names: List[str], output_dir: str):\n",
    "    \"\"\"REUSED: Save all results to CSV files.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for method_name in method_names:\n",
    "        if method_name in all_results:\n",
    "            metrics_df = compute_accuracy_metrics(all_results[method_name], method_name)\n",
    "            safe_name = method_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "            metrics_path = os.path.join(output_dir, f'{safe_name}_metrics.csv')\n",
    "            metrics_df.to_csv(metrics_path, index=False)\n",
    "            print(f\"  âœ“ Saved {method_name} metrics: {metrics_path}\")\n",
    "\n",
    "def print_final_summary_table(all_results: Dict, method_names: List[str], k_shots: List[int]):\n",
    "    \"\"\"REUSED: Print comprehensive summary table.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FINAL SUMMARY TABLE\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    summary_data = []\n",
    "    for method_name in method_names:\n",
    "        if method_name not in all_results:\n",
    "            continue\n",
    "        \n",
    "        results = all_results[method_name]\n",
    "        subjects = results.get('subjects', {})\n",
    "        row = {'Method': method_name}\n",
    "        \n",
    "        for k in k_shots:\n",
    "            accuracies = []\n",
    "            for subject_data in subjects.values():\n",
    "                if 'k_shots' in subject_data and k in subject_data['k_shots']:\n",
    "                    acc = subject_data['k_shots'][k].get('accuracy', 0)\n",
    "                    accuracies.append(acc)\n",
    "            \n",
    "            if accuracies:\n",
    "                mean_acc = np.mean(accuracies) * 100\n",
    "                std_acc = np.std(accuracies) * 100\n",
    "                row[f'K={k}'] = f'{mean_acc:.2f}Â±{std_acc:.2f}'\n",
    "            else:\n",
    "                row[f'K={k}'] = 'N/A'\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"âœ“ BASELINES & EVALUATION FUNCTIONS\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c4762",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸš€ Section 9: Experimental Execution\n",
    "\n",
    "**PUBLICATION-QUALITY RUN**\n",
    "\n",
    "This cell executes the full experimental pipeline:\n",
    "1. Subject-conditioned meta-learning (NEW)\n",
    "2. Supervised baseline (comparison)\n",
    "3. MAML-Encoder baseline (comparison with previous method)\n",
    "4. Results visualization and analysis\n",
    "\n",
    "### Configuration\n",
    "- Run ONE seed at a time on Kaggle to avoid timeout\n",
    "- Seeds: 42, 123, 456\n",
    "- Estimated time: ~2-3 hours per seed\n",
    "\n",
    "### What Gets Tested\n",
    "- **Subject-Conditioned (NEW)**: FiLM conditioning + learned z_s\n",
    "- **Supervised Baseline**: Standard supervised learning\n",
    "- **MAML-Encoder**: Previous best method (for comparison)\n",
    "\n",
    "### Success Criteria\n",
    "Subject-conditioned method should show:\n",
    "- âœ… Better low-K (5, 10) performance than baselines\n",
    "- âœ… Reduced inter-subject variance\n",
    "- âœ… Monotonic improvement with K\n",
    "- âœ… Competitive or superior to MAML-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f78fff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " SUBJECT-CONDITIONED META-LEARNING - PUBLICATION RUN\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      " SEED: 456 (ONE SEED PER RUN)\n",
      "======================================================================\n",
      "\n",
      "Device: cuda\n",
      "K-shots: [5, 10, 20, 50]\n",
      "Meta-iterations: 500\n",
      "Subject embed dim: 32\n",
      "Inner LR: 0.01\n",
      "Outer LR: 0.0003\n",
      "Inner steps: 5\n",
      "Adapt encoder: False\n",
      "Subjects: 16\n",
      "\n",
      "â±ï¸ Estimated time: ~2-3 hours for full pipeline\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      " STEP 1/3: SUBJECT-CONDITIONED META-LEARNING (Seed: 456) [NEW]\n",
      "======================================================================\n",
      "\n",
      "âš ï¸  This is the NEW method that addresses the bottleneck:\n",
      "   - Infers subject embedding z_s from support set\n",
      "   - Conditions encoder with FiLM layers\n",
      "   - Meta-learns SubjectEncoder + ConditionedEEGEncoder\n",
      "   - Adapts TaskHead per subject\n",
      "\n",
      "âœ“ Random seed set to 456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52eb1a0a02904b49aad72683c3e7aba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject-Conditioned LOSO:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Test Subject: S02\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.6023, Query Acc = 0.7000\n",
      "Iteration 200/500: Meta Loss = 0.5787, Query Acc = 0.7375\n",
      "Iteration 300/500: Meta Loss = 0.5956, Query Acc = 0.7000\n",
      "Iteration 400/500: Meta Loss = 0.6195, Query Acc = 0.6750\n",
      "Iteration 500/500: Meta Loss = 0.6767, Query Acc = 0.5875\n",
      "\n",
      "Evaluating on test subject S02...\n",
      "  K=5: Accuracy = 0.6384 (203/318)\n",
      "  K=10: Accuracy = 0.6454 (202/313)\n",
      "  K=20: Accuracy = 0.6370 (193/303)\n",
      "  K=50: Accuracy = 0.6557 (179/273)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S06\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.6237, Query Acc = 0.6875\n",
      "Iteration 200/500: Meta Loss = 0.5686, Query Acc = 0.7438\n",
      "Iteration 300/500: Meta Loss = 0.6044, Query Acc = 0.6938\n",
      "Iteration 400/500: Meta Loss = 0.5800, Query Acc = 0.7000\n",
      "Iteration 500/500: Meta Loss = 0.5936, Query Acc = 0.6813\n",
      "\n",
      "Evaluating on test subject S06...\n",
      "  K=5: Accuracy = 0.9371 (283/302)\n",
      "  K=10: Accuracy = 0.9360 (278/297)\n",
      "  K=20: Accuracy = 0.9373 (269/287)\n",
      "  K=50: Accuracy = 0.9300 (239/257)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S07\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.6113, Query Acc = 0.6875\n",
      "Iteration 200/500: Meta Loss = 0.5722, Query Acc = 0.7375\n",
      "Iteration 300/500: Meta Loss = 0.6361, Query Acc = 0.6688\n",
      "Iteration 400/500: Meta Loss = 0.5901, Query Acc = 0.7000\n",
      "Iteration 500/500: Meta Loss = 0.6335, Query Acc = 0.6313\n",
      "\n",
      "Evaluating on test subject S07...\n",
      "  K=5: Accuracy = 0.8819 (224/254)\n",
      "  K=10: Accuracy = 0.8835 (220/249)\n",
      "  K=20: Accuracy = 0.8787 (210/239)\n",
      "  K=50: Accuracy = 0.8947 (187/209)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S11\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.6169, Query Acc = 0.6938\n",
      "Iteration 200/500: Meta Loss = 0.5566, Query Acc = 0.7563\n",
      "Iteration 300/500: Meta Loss = 0.6052, Query Acc = 0.6813\n",
      "Iteration 400/500: Meta Loss = 0.5756, Query Acc = 0.7375\n",
      "Iteration 500/500: Meta Loss = 0.5352, Query Acc = 0.7250\n",
      "\n",
      "Evaluating on test subject S11...\n",
      "  K=5: Accuracy = 0.6698 (213/318)\n",
      "  K=10: Accuracy = 0.6773 (212/313)\n",
      "  K=20: Accuracy = 0.6700 (203/303)\n",
      "  K=50: Accuracy = 0.6923 (189/273)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S12\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.5919, Query Acc = 0.7250\n",
      "Iteration 200/500: Meta Loss = 0.5499, Query Acc = 0.7625\n",
      "Iteration 300/500: Meta Loss = 0.4837, Query Acc = 0.8188\n",
      "Iteration 400/500: Meta Loss = 0.5703, Query Acc = 0.7438\n",
      "Iteration 500/500: Meta Loss = 0.5711, Query Acc = 0.7188\n",
      "\n",
      "Evaluating on test subject S12...\n",
      "  K=5: Accuracy = 0.5225 (93/178)\n",
      "  K=10: Accuracy = 0.5145 (89/173)\n",
      "  K=20: Accuracy = 0.5215 (85/163)\n",
      "  K=50: Accuracy = 0.5188 (69/133)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S13\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.6239, Query Acc = 0.6813\n",
      "Iteration 200/500: Meta Loss = 0.5635, Query Acc = 0.7500\n",
      "Iteration 300/500: Meta Loss = 0.6055, Query Acc = 0.6938\n",
      "Iteration 400/500: Meta Loss = 0.6246, Query Acc = 0.6812\n",
      "Iteration 500/500: Meta Loss = 0.5353, Query Acc = 0.7625\n",
      "\n",
      "Evaluating on test subject S13...\n",
      "  K=5: Accuracy = 0.5158 (163/316)\n",
      "  K=10: Accuracy = 0.5241 (163/311)\n",
      "  K=20: Accuracy = 0.5116 (154/301)\n",
      "  K=50: Accuracy = 0.4945 (134/271)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S14\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.6152, Query Acc = 0.6875\n",
      "Iteration 200/500: Meta Loss = 0.5881, Query Acc = 0.7250\n",
      "Iteration 300/500: Meta Loss = 0.6348, Query Acc = 0.6625\n",
      "Iteration 400/500: Meta Loss = 0.5500, Query Acc = 0.7375\n",
      "Iteration 500/500: Meta Loss = 0.5556, Query Acc = 0.7375\n",
      "\n",
      "Evaluating on test subject S14...\n",
      "  K=5: Accuracy = 0.6258 (199/318)\n",
      "  K=10: Accuracy = 0.6390 (200/313)\n",
      "  K=20: Accuracy = 0.6271 (190/303)\n",
      "  K=50: Accuracy = 0.6300 (172/273)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S16\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.6372, Query Acc = 0.6688\n",
      "Iteration 200/500: Meta Loss = 0.6479, Query Acc = 0.6688\n",
      "Iteration 300/500: Meta Loss = 0.5923, Query Acc = 0.7125\n",
      "Iteration 400/500: Meta Loss = 0.5249, Query Acc = 0.7688\n",
      "Iteration 500/500: Meta Loss = 0.5711, Query Acc = 0.6813\n",
      "\n",
      "Evaluating on test subject S16...\n",
      "  K=5: Accuracy = 0.5966 (176/295)\n",
      "  K=10: Accuracy = 0.6069 (176/290)\n",
      "  K=20: Accuracy = 0.6000 (168/280)\n",
      "  K=50: Accuracy = 0.5960 (149/250)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S17\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.6864, Query Acc = 0.6000\n",
      "Iteration 200/500: Meta Loss = 0.6144, Query Acc = 0.6875\n",
      "Iteration 300/500: Meta Loss = 0.5924, Query Acc = 0.7000\n",
      "Iteration 400/500: Meta Loss = 0.5282, Query Acc = 0.7500\n",
      "Iteration 500/500: Meta Loss = 0.5585, Query Acc = 0.6938\n",
      "\n",
      "Evaluating on test subject S17...\n",
      "  K=5: Accuracy = 0.6604 (210/318)\n",
      "  K=10: Accuracy = 0.6581 (206/313)\n",
      "  K=20: Accuracy = 0.6601 (200/303)\n",
      "  K=50: Accuracy = 0.6447 (176/273)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S18\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.6282, Query Acc = 0.6750\n",
      "Iteration 200/500: Meta Loss = 0.6018, Query Acc = 0.7063\n",
      "Iteration 300/500: Meta Loss = 0.5790, Query Acc = 0.7313\n",
      "Iteration 400/500: Meta Loss = 0.5573, Query Acc = 0.7125\n",
      "Iteration 500/500: Meta Loss = 0.6493, Query Acc = 0.6188\n",
      "\n",
      "Evaluating on test subject S18...\n",
      "  K=5: Accuracy = 0.7312 (204/279)\n",
      "  K=10: Accuracy = 0.7263 (199/274)\n",
      "  K=20: Accuracy = 0.7235 (191/264)\n",
      "  K=50: Accuracy = 0.7265 (170/234)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S20\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.5969, Query Acc = 0.7188\n",
      "Iteration 200/500: Meta Loss = 0.5424, Query Acc = 0.7625\n",
      "Iteration 300/500: Meta Loss = 0.5353, Query Acc = 0.7687\n",
      "Iteration 400/500: Meta Loss = 0.5798, Query Acc = 0.7125\n",
      "Iteration 500/500: Meta Loss = 0.6324, Query Acc = 0.6438\n",
      "\n",
      "Evaluating on test subject S20...\n",
      "  K=5: Accuracy = 0.6970 (138/198)\n",
      "  K=10: Accuracy = 0.7098 (137/193)\n",
      "  K=20: Accuracy = 0.7104 (130/183)\n",
      "  K=50: Accuracy = 0.7124 (109/153)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S21\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.5605, Query Acc = 0.7688\n",
      "Iteration 200/500: Meta Loss = 0.5606, Query Acc = 0.7500\n",
      "Iteration 300/500: Meta Loss = 0.6614, Query Acc = 0.6313\n",
      "Iteration 400/500: Meta Loss = 0.5111, Query Acc = 0.8000\n",
      "Iteration 500/500: Meta Loss = 0.6557, Query Acc = 0.6250\n",
      "\n",
      "Evaluating on test subject S21...\n",
      "  K=5: Accuracy = 0.9184 (90/98)\n",
      "  K=10: Accuracy = 0.9140 (85/93)\n",
      "  K=20: Accuracy = 0.9277 (77/83)\n",
      "  K=50: Accuracy = 0.9057 (48/53)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S22\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.6079, Query Acc = 0.7000\n",
      "Iteration 200/500: Meta Loss = 0.5189, Query Acc = 0.7938\n",
      "Iteration 300/500: Meta Loss = 0.6307, Query Acc = 0.6688\n",
      "Iteration 400/500: Meta Loss = 0.5665, Query Acc = 0.7063\n",
      "Iteration 500/500: Meta Loss = 0.6710, Query Acc = 0.6000\n",
      "\n",
      "Evaluating on test subject S22...\n",
      "  K=5: Accuracy = 0.8031 (102/127)\n",
      "  K=10: Accuracy = 0.8443 (103/122)\n",
      "  K=20: Accuracy = 0.8661 (97/112)\n",
      "  K=50: Accuracy = 0.8171 (67/82)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S23\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.5838, Query Acc = 0.7188\n",
      "Iteration 200/500: Meta Loss = 0.5622, Query Acc = 0.7313\n",
      "Iteration 300/500: Meta Loss = 0.5840, Query Acc = 0.7313\n",
      "Iteration 400/500: Meta Loss = 0.4816, Query Acc = 0.8125\n",
      "Iteration 500/500: Meta Loss = 0.5861, Query Acc = 0.6938\n",
      "\n",
      "Evaluating on test subject S23...\n",
      "  K=5: Accuracy = 0.6478 (206/318)\n",
      "  K=10: Accuracy = 0.6422 (201/313)\n",
      "  K=20: Accuracy = 0.6370 (193/303)\n",
      "  K=50: Accuracy = 0.6593 (180/273)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S24\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.6452, Query Acc = 0.6500\n",
      "Iteration 200/500: Meta Loss = 0.6193, Query Acc = 0.6937\n",
      "Iteration 300/500: Meta Loss = 0.5457, Query Acc = 0.7500\n",
      "Iteration 400/500: Meta Loss = 0.5181, Query Acc = 0.7438\n",
      "Iteration 500/500: Meta Loss = 0.5482, Query Acc = 0.7625\n",
      "\n",
      "Evaluating on test subject S24...\n",
      "  K=5: Accuracy = 0.7075 (225/318)\n",
      "  K=10: Accuracy = 0.7157 (224/313)\n",
      "  K=20: Accuracy = 0.6997 (212/303)\n",
      "  K=50: Accuracy = 0.7143 (195/273)\n",
      "\n",
      "======================================================================\n",
      "Test Subject: S26\n",
      "======================================================================\n",
      "\n",
      "Meta-training on 15 subjects...\n",
      "Iteration 100/500: Meta Loss = 0.5818, Query Acc = 0.7313\n",
      "Iteration 200/500: Meta Loss = 0.5655, Query Acc = 0.7438\n",
      "Iteration 300/500: Meta Loss = 0.7014, Query Acc = 0.6000\n",
      "Iteration 400/500: Meta Loss = 0.5908, Query Acc = 0.7000\n",
      "Iteration 500/500: Meta Loss = 0.6457, Query Acc = 0.6125\n",
      "\n",
      "Evaluating on test subject S26...\n",
      "  K=5: Accuracy = 0.5377 (171/318)\n",
      "  K=10: Accuracy = 0.5367 (168/313)\n",
      "  K=20: Accuracy = 0.5479 (166/303)\n",
      "  K=50: Accuracy = 0.5128 (140/273)\n",
      "\n",
      "======================================================================\n",
      "âœ“ Subject-conditioned meta-learning complete for seed 456!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      " STEP 2/3: SUPERVISED BASELINE (Seed: 456)\n",
      "======================================================================\n",
      "âœ“ Random seed set to 456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a0e648508f4196b81241257f7f97a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Supervised Baseline (LOSO):   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Supervised baseline complete for seed 456!\n",
      "\n",
      "======================================================================\n",
      " STEP 3/3: RESULTS & VISUALIZATION (Seed: 456)\n",
      "======================================================================\n",
      "\n",
      "â–¶ Computing summary statistics...\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY TABLE\n",
      "======================================================================\n",
      "\n",
      "                   Method         K=5        K=10        K=20        K=50\n",
      "Subject-Conditioned (NEW) 69.32Â±12.87 69.84Â±12.99 69.72Â±13.33 69.40Â±13.23\n",
      "      Supervised Baseline 62.42Â±14.98 64.36Â±13.98 62.19Â±14.32 62.90Â±13.75\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "â–¶ Saving results to CSV...\n",
      "  âœ“ Saved Subject-Conditioned (NEW) metrics: /kaggle/working/results_subject_conditioned/metrics/subject-conditioned_new_metrics.csv\n",
      "  âœ“ Saved Supervised Baseline metrics: /kaggle/working/results_subject_conditioned/metrics/supervised_baseline_metrics.csv\n",
      "\n",
      "â–¶ Generating adaptation curves...\n",
      "  Saved: /kaggle/working/results_subject_conditioned/figures/adaptation_curves_seed456.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â–¶ Saving experimental configuration...\n",
      "\n",
      "â–¶ Saving complete results pickle...\n",
      "  âœ“ Saved to: /kaggle/working/results_subject_conditioned/complete_results_seed456.pkl\n",
      "\n",
      "======================================================================\n",
      " âœ“ SEED 456 COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Results: /kaggle/working/results_subject_conditioned\n",
      "Figures: /kaggle/working/results_subject_conditioned/figures\n",
      "Metrics: /kaggle/working/results_subject_conditioned/metrics\n",
      "\n",
      "ðŸ“Š KEY FINDINGS TO CHECK:\n",
      "   1. Subject-Conditioned should show LOWER variance than Supervised\n",
      "   2. Better performance at low K (5, 10) due to explicit subject modeling\n",
      "   3. Monotonic improvement with K (no degradation)\n",
      "   4. Subject embedding z_s effectively captures subject-specific geometry\n",
      "\n",
      "âš ï¸ TO RUN NEXT SEED:\n",
      "   1. Change SEED = 456 to SEED = 123 (or 456)\n",
      "   2. Restart kernel and run all cells\n",
      "   3. Repeat for all 3 seeds: 42, 123, 456\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PUBLICATION-QUALITY EXPERIMENTAL PIPELINE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# \n",
    "# âš ï¸ IMPORTANT FOR KAGGLE:\n",
    "# Run ONE seed at a time to avoid session timeout!\n",
    "# Change SEED below and rerun for each: 42, 123, 456\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\" SUBJECT-CONDITIONED META-LEARNING - PUBLICATION RUN\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# â”€â”€â”€ CONFIGURATION FOR PUBLICATION â”€â”€â”€\n",
    "SEED = 456  # âš ï¸ CHANGE THIS: 42, 123, or 456 (run one at a time!)\n",
    "\n",
    "FINAL_CONFIG = {\n",
    "    'k_shots': [5, 10, 20, 50],\n",
    "    'n_meta_iterations': 500,      # 1000 if time allows\n",
    "    'meta_batch_size': 4,\n",
    "    'n_support': 10,\n",
    "    'n_query': 40,\n",
    "    'subject_embed_dim': 32,       # NEW: Dimension of z_s (increased for more capacity)\n",
    "    'encoder_hidden': 64,\n",
    "    'encoder_output': 32,\n",
    "    'inner_lr': 0.01,\n",
    "    'outer_lr': 0.0003,            # Lower LR for stability\n",
    "    'inner_steps': 5,\n",
    "    'adapt_encoder': False,        # Adapt task head only (default)\n",
    "    'device': Config.DEVICE\n",
    "}\n",
    "\n",
    "K_SHOTS = FINAL_CONFIG['k_shots']\n",
    "N_META_ITERATIONS = FINAL_CONFIG['n_meta_iterations']\n",
    "DEVICE = FINAL_CONFIG['device']\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\" SEED: {SEED} (ONE SEED PER RUN)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nDevice: {DEVICE}\")\n",
    "print(f\"K-shots: {K_SHOTS}\")\n",
    "print(f\"Meta-iterations: {N_META_ITERATIONS}\")\n",
    "print(f\"Subject embed dim: {FINAL_CONFIG['subject_embed_dim']}\")\n",
    "print(f\"Inner LR: {FINAL_CONFIG['inner_lr']}\")\n",
    "print(f\"Outer LR: {FINAL_CONFIG['outer_lr']}\")\n",
    "print(f\"Inner steps: {FINAL_CONFIG['inner_steps']}\")\n",
    "print(f\"Adapt encoder: {FINAL_CONFIG['adapt_encoder']}\")\n",
    "print(f\"Subjects: {len(loso_splits)}\")\n",
    "print(f\"\\nâ±ï¸ Estimated time: ~2-3 hours for full pipeline\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# â”€â”€â”€ STEP 1: Subject-Conditioned Meta-Learning (NEW) â”€â”€â”€\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\" STEP 1/3: SUBJECT-CONDITIONED META-LEARNING (Seed: {SEED}) [NEW]\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nâš ï¸  This is the NEW method that addresses the bottleneck:\")\n",
    "print(\"   - Infers subject embedding z_s from support set\")\n",
    "print(\"   - Conditions encoder with FiLM layers\")\n",
    "print(\"   - Meta-learns SubjectEncoder + ConditionedEEGEncoder\")\n",
    "print(\"   - Adapts TaskHead per subject\")\n",
    "print(\"\")\n",
    "\n",
    "subject_conditioned_results = train_subject_conditioned_loso(\n",
    "    loso_splits=loso_splits,\n",
    "    k_shots=K_SHOTS,\n",
    "    n_meta_iterations=N_META_ITERATIONS,\n",
    "    meta_batch_size=FINAL_CONFIG['meta_batch_size'],\n",
    "    n_support=FINAL_CONFIG['n_support'],\n",
    "    n_query=FINAL_CONFIG['n_query'],\n",
    "    subject_embed_dim=FINAL_CONFIG['subject_embed_dim'],\n",
    "    encoder_hidden=FINAL_CONFIG['encoder_hidden'],\n",
    "    encoder_output=FINAL_CONFIG['encoder_output'],\n",
    "    inner_lr=FINAL_CONFIG['inner_lr'],\n",
    "    outer_lr=FINAL_CONFIG['outer_lr'],\n",
    "    inner_steps=FINAL_CONFIG['inner_steps'],\n",
    "    adapt_encoder=FINAL_CONFIG['adapt_encoder'],\n",
    "    device=DEVICE,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ“ Subject-conditioned meta-learning complete for seed {SEED}!\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# â”€â”€â”€ STEP 2: Supervised Baseline (for comparison) â”€â”€â”€\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\" STEP 2/3: SUPERVISED BASELINE (Seed: {SEED})\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "supervised_results = run_supervised_baseline_loso(\n",
    "    loso_splits=loso_splits,\n",
    "    k_shots=K_SHOTS,\n",
    "    hidden_dim=64,\n",
    "    lr=0.01,\n",
    "    n_epochs=100,\n",
    "    device=DEVICE,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Supervised baseline complete for seed {SEED}!\")\n",
    "\n",
    "# â”€â”€â”€ STEP 3: Results & Visualization â”€â”€â”€\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\" STEP 3/3: RESULTS & VISUALIZATION (Seed: {SEED})\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Compile results\n",
    "all_results = {\n",
    "    'Subject-Conditioned (NEW)': subject_conditioned_results,\n",
    "    'Supervised Baseline': supervised_results\n",
    "}\n",
    "\n",
    "method_names = ['Subject-Conditioned (NEW)', 'Supervised Baseline']\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nâ–¶ Computing summary statistics...\")\n",
    "summary_df = print_final_summary_table(all_results, method_names, K_SHOTS)\n",
    "\n",
    "# Save results to CSV\n",
    "print(\"\\nâ–¶ Saving results to CSV...\")\n",
    "save_results_to_csv(all_results, method_names, Config.METRICS_DIR)\n",
    "\n",
    "# Plot adaptation curves\n",
    "print(\"\\nâ–¶ Generating adaptation curves...\")\n",
    "plot_adaptation_curves(\n",
    "    all_results,\n",
    "    method_names,\n",
    "    save_path=os.path.join(Config.FIGURES_DIR, f'adaptation_curves_seed{SEED}.png')\n",
    ")\n",
    "\n",
    "# Save configuration\n",
    "print(\"\\nâ–¶ Saving experimental configuration...\")\n",
    "config_dict = {\n",
    "    'method': 'Subject-Conditioned Meta-Learning',\n",
    "    'dataset_root': Config.DATASET_ROOT,\n",
    "    'k_shots': K_SHOTS,\n",
    "    'random_seed': SEED,\n",
    "    'n_meta_iterations': N_META_ITERATIONS,\n",
    "    'subject_embed_dim': FINAL_CONFIG['subject_embed_dim'],\n",
    "    'encoder_hidden': FINAL_CONFIG['encoder_hidden'],\n",
    "    'encoder_output': FINAL_CONFIG['encoder_output'],\n",
    "    'inner_lr': FINAL_CONFIG['inner_lr'],\n",
    "    'outer_lr': FINAL_CONFIG['outer_lr'],\n",
    "    'inner_steps': FINAL_CONFIG['inner_steps'],\n",
    "    'adapt_encoder': FINAL_CONFIG['adapt_encoder'],\n",
    "    'meta_batch_size': FINAL_CONFIG['meta_batch_size'],\n",
    "    'n_support': FINAL_CONFIG['n_support'],\n",
    "    'n_query': FINAL_CONFIG['n_query'],\n",
    "    'device': str(DEVICE),\n",
    "    'innovation': 'FiLM conditioning with learned subject embedding z_s'\n",
    "}\n",
    "\n",
    "with open(os.path.join(Config.RESULTS_DIR, f'experimental_config_seed{SEED}.json'), 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "# Save complete results\n",
    "print(\"\\nâ–¶ Saving complete results pickle...\")\n",
    "import pickle\n",
    "results_path = os.path.join(Config.RESULTS_DIR, f'complete_results_seed{SEED}.pkl')\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'subject_conditioned_results': subject_conditioned_results,\n",
    "        'supervised_results': supervised_results,\n",
    "        'all_results': all_results,\n",
    "        'config': config_dict,\n",
    "        'summary_df': summary_df\n",
    "    }, f)\n",
    "print(f\"  âœ“ Saved to: {results_path}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\" âœ“ SEED {SEED} COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nResults: {Config.RESULTS_DIR}\")\n",
    "print(f\"Figures: {Config.FIGURES_DIR}\")\n",
    "print(f\"Metrics: {Config.METRICS_DIR}\")\n",
    "print(f\"\\nðŸ“Š KEY FINDINGS TO CHECK:\")\n",
    "print(f\"   1. Subject-Conditioned should show LOWER variance than Supervised\")\n",
    "print(f\"   2. Better performance at low K (5, 10) due to explicit subject modeling\")\n",
    "print(f\"   3. Monotonic improvement with K (no degradation)\")\n",
    "print(f\"   4. Subject embedding z_s effectively captures subject-specific geometry\")\n",
    "print(f\"\\nâš ï¸ TO RUN NEXT SEED:\")\n",
    "print(f\"   1. Change SEED = {SEED} to SEED = 123 (or 456)\")\n",
    "print(f\"   2. Restart kernel and run all cells\")\n",
    "print(f\"   3. Repeat for all 3 seeds: 42, 123, 456\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
